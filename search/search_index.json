{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Browsertrix Cloud official user guide and developer docs. These docs will contain the following sections.</p> <ul> <li>Deployment Guide \u2014 How to install and deploy Browsertrix Cloud on your local machine, or in the cloud.</li> <li>Developer Docs \u2014 Information on developing Browsertrix Cloud itself.</li> <li>User Guide \u2014 Instructions and reference for using Browsertrix Cloud.</li> </ul> <p>If you are unfamiliar with Browsertrix Cloud, please check out our website, or the main repository at https://github.com/webrecorder/browsertrix-cloud</p> <p>Our docs are still under construction. If you find something missing, chances are we haven't gotten around to writing that part yet. If you find typos or something isn't clear or seems incorrect, please open an issue and we'll try to make sure that your questions get answered here in the future!</p>"},{"location":"deploy/","title":"Deploying Browsertrix Cloud","text":"<p>Browsertrix Cloud is designed to be a cloud-native application running in Kubernetes.</p> <p>However, despite the name, it is perfectly reasonable (and easy!) to deploy Browsertrix Cloud locally using one of the many available local Kubernetes options.</p> <p>The main requirements for Browsertrix Cloud are:</p> <ul> <li>A Kubernetes Cluster</li> <li>Helm 3 (package manager for Kubernetes)</li> </ul> <p>We have prepared a Local Deployment Guide which covers several options for testing Browsertrix Cloud locally on a single machine, as well as a Production (Self-Hosted and Cloud) Deployment guides to help with setting up Browsertrix Cloud for different production scenarios.</p>"},{"location":"deploy/local/","title":"Local Deployment","text":"<p>To try out the latest release of Browsertrix Cloud on your local machine, you'll first need to have a working Kubernetes cluster.</p>"},{"location":"deploy/local/#installing-kubernetes","title":"Installing Kubernetes","text":"<p>Before running Browsertrix Cloud, you'll need to set up a running Kubernetes cluster.</p> <p>Today, there are numerous ways to deploy Kubernetes fairly easily, and we recommend trying one of the single-node options, which include Docker Desktop, microk8s, minikube and k3s.</p> <p>The instructions below assume you have the local package managers for your platform (eg. <code>brew</code> for macOS, <code>choco</code> for Windows, etc...) already installed.</p> <p>Cloning the repository at https://github.com/webrecorder/browsertrix-cloud is only needed to access additional configuration files.</p> <p>Here are some environment specific instructions for setting up a local cluster from different Kubernetes vendors:</p> Docker Desktop (recommended for macOS and Windows) <p>For macOS and Windows, we recommend testing out Browsertrix Cloud using Kubernetes support in Docker Desktop as that will be one of the simplest options.</p> <ol> <li> <p>Install Docker Desktop if not already installed.</p> </li> <li> <p>From the Dashboard app, ensure <code>Enable Kubernetes</code> is checked from the Preferences screen.</p> </li> <li> <p>Restart Docker Desktop if asked, and wait for it to fully restart.</p> </li> <li> <p>Install Helm, which can be installed with <code>brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> </ol> MicroK8S (recommended for Ubuntu) <p>For Ubuntu and other linux distros, we recommend using MicroK8S for both local deployment and production.</p> <ol> <li> <p>Install MicroK8s, by running <code>sudo snap install microk8s --classic</code> see more detailed instructions here or alternate installation instructions here</p> </li> <li> <p>Install the following addons <code>microk8s enable dns hostpath-storage registry helm3</code>. (For production, also add <code>ingress cert-manager</code> to the list of addons)</p> </li> <li> <p>Wait for add-ons to finish installing with <code>microk8s status --wait-ready</code></p> </li> </ol> <p>Note: microk8s comes with its own version helm, so you don't need to install it separately. Replace <code>helm</code> with <code>microk8s helm3</code> in the subsequent instructions below.</p> Minikube (Windows, macOS, or Linux) <ol> <li> <p>Install Minikube following installation instructions, eg. <code>brew install minikube</code>.    Note that Minikube also requires Docker or another container management system to be installed as well.</p> </li> <li> <p>Install Helm, which can be installed with <code>brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> </ol> K3S (recommended for non-Ubuntu Linux) <ol> <li> <p>Install K3s as per the instructions</p> </li> <li> <p>Install Helm, which can be installed with <code>brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> <li> <p>Set <code>KUBECONFIG</code> to point to the config for K3S: <code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml</code> to ensure Helm will use the correct version.</p> </li> </ol>"},{"location":"deploy/local/#launching-browsertrix-cloud-with-helm","title":"Launching Browsertrix Cloud with Helm","text":"<p>Once you have a running Kubernetes cluster with one of the options above, and Helm 3 installed, you can install the latest release of Browsertrix Cloud directly from the latest GitHub release.</p> <p></p> <pre><code>helm upgrade --install btrix \\\nhttps://github.com/webrecorder/browsertrix-cloud/releases/download/VERSION/browsertrix-cloud-VERSION.tgz\n</code></pre> MicroK8S <p>If using microk8s, the command will be:</p> <p></p> <pre><code>microk8s helm3 upgrade --install btrix \\\nhttps://github.com/webrecorder/browsertrix-cloud/releases/download/VERSION/browsertrix-cloud-VERSION.tgz\n</code></pre> <p>Subsequent commands will also use <code>microk8s helm3</code> instead of <code>helm</code>.</p> <p>The default setup includes the full Browsertrix Cloud system, with frontend, backend api, db (via MongoDB) and storage (via Minio)</p> <p>An admin user with name <code>admin@example.com</code> and password <code>PASSW0RD!</code> will be automatically created.</p> <p>With Helm, additional YAML files can be added to further override previous settings.</p> <p>Some possible settings can be changed are found in chart/examples/local-config.yaml.</p> <p>For example, to change the default superadmin, uncomment the <code>superadmin</code> block in <code>local-config.yaml</code>, and then change the username (<code>admin@example.com</code>) and password (<code>PASSW0RD!</code>) to different values. (The admin username and password will be updated with each deployment). To change the local port, change <code>local_service_port</code> setting.</p> <p>You can then redeploy with these additional settings by running:</p> <p></p> <pre><code>helm upgrade --install btrix https://github.com/webrecorder/browsertrix-cloud/releases/download/VERSION/browsertrix-cloud-VERSION.tgz \\\n-f ./chart/examples/local-config.yaml\n</code></pre> <p>The above examples assumes running from a cloned Browsertrix Cloud repo, however the config file can be saved anywhere and specified with <code>-f &lt;extra-config.yaml&gt;</code>.</p>"},{"location":"deploy/local/#waiting-for-cluster-to-start","title":"Waiting for Cluster to Start","text":"<p>After running the helm command, you should see something like:</p> <pre><code>Release \"btrix\" does not exist. Installing it now.\nNAME: btrix\nLAST DEPLOYED: &lt;time&gt;\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>After that, especially on first run, it may take a few minutes for the Browsertrix Cloud cluster to start, as all images need to be downloaded locally.</p> <p>You can try running the command: <code>kubectl wait --for=condition=ready pod --all --timeout=300s</code> to wait for all pods to be initialized.</p> <p>The command will exit when all pods have been loaded, or if there is an error and it times out.</p> <p>If the command succeeds, you should be able to access Browsertrix Cloud by loading: http://localhost:30870/ in your browser.</p> Minikube (on macOS) <p>When using Minikube on a macOS, the port will not be 30870. Instead, Minikube opens a tunnel to a random port, obtained by running <code>minikube service browsertrix-cloud-frontend --url</code> in a separate terminal. Use the provided URL (in the format <code>http://127.0.0.1:&lt;TUNNEL_PORT&gt;</code>) instead.</p>"},{"location":"deploy/local/#debugging-pod-issues","title":"Debugging Pod Issues","text":"<p>If this command fails, you can also run <code>kubectl get pods</code> to see the status of each of the pods.</p> <p>There should be 4 pods listed: backend, frontend, minio and mongodb. If any one is not ready for a while, something may be wrong.</p> <p>To get more details about why a pod has not started, you can run <code>kubectl describe &lt;podname&gt;</code> and see the latest status at the bottom.</p> <p>Often, the error may be obvious, such as failed to pull an image.</p> <p>If the pod is running, or previously ran, you can also get the logs from the container by running <code>kubectl logs &lt;podname&gt;</code></p> <p>The outputs of these commands will be helpful if you'd like to report an issue on GitHub</p>"},{"location":"deploy/local/#updating-the-cluster","title":"Updating the Cluster","text":"<p>To update the cluster, for example to update to new version <code>NEWVERSION</code>, re-run the same command again, which will pull the latest images. In this way, you can upgrade to the latest release of Browsertrix Cloud. The upgrade will preserve the database and current archives.</p> <pre><code>helm upgrade --install btrix https://github.com/webrecorder/browsertrix-cloud/releases/download/NEWVERSION/browsertrix-cloud-NEWVERSION.tgz\n</code></pre>"},{"location":"deploy/local/#uninstalling","title":"Uninstalling","text":"<p>To uninstall, run <code>helm uninstall btrix</code>.</p> <p>By default, the database + storage volumes are not automatically deleted, so you can run <code>helm upgrade ...</code> again to restart the cluster in its current state.</p> <p>If you are upgrading from a previous version, and run into issues with <code>helm upgrade ...</code>, we recommend uninstalling and then re-running upgrade.</p>"},{"location":"deploy/local/#deleting-all-data","title":"Deleting all Data","text":"<p>To fully delete all persistent data (db + archives) created in the cluster, also run <code>kubectl delete pvc --all</code> after uninstalling.</p>"},{"location":"deploy/local/#deploying-for-local-development","title":"Deploying for Local Development","text":"<p>These instructions are intended for deploying the cluster from the latest releases published on GitHub. See setting up cluster for local development for additional customizations related to developing Browsertrix Cloud and deploying from local images.</p>"},{"location":"deploy/remote/","title":"Remote: Self-Hosted and Cloud","text":"<p>For remote and hosted deployments (both on a single machine or in the cloud), the only requirement is to have a designed domain and (strongly recommended, but not required) second domain for signing web archives. </p> <p>We are also experimenting with Ansible playbooks for cloud deployment setups.</p> <p>The production deployments also allow using an external mongodb server, and/or external S3-compatible storage instead of the bundled minio.</p>"},{"location":"deploy/remote/#single-machine-deployment-with-microk8s","title":"Single Machine Deployment with MicroK8S","text":"<p>For a single-machine remote deployment, we recommend using MicroK8s.</p> <ol> <li> <p>Install MicroK8S, as suggested in the local deployment guide and ensure the <code>ingress</code> and <code>cert-manager</code> addons are also enabled.</p> </li> <li> <p>Copy <code>cp ./chart/examples/microk8s-hosted.yaml ./chart/my-config.yaml</code> to make local changes.</p> </li> <li> <p>Set the <code>ingress.host</code>, <code>ingress.cert_email</code> and <code>signing.host</code> fields in <code>./chart/my-config.yaml</code> to your host and domain</p> </li> <li> <p>Set the super-admin username and password, and mongodb username and password in <code>./chart/my-config.yaml</code></p> </li> <li> <p>Run with:</p> </li> </ol> <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/my-config.yaml btrix ./chart/\n</code></pre>"},{"location":"deploy/remote/#single-machine-deployment-with-k3s","title":"Single Machine Deployment with k3s","text":"<p>Another option for a single-machine remote deployment is k3s</p> <ol> <li> <p>Install K3s, as suggested in the local deployment guide. Make sure to disable traefik which can be done by adding <code>--no-deploy traefik</code> to the <code>systemd</code> unit when installing k3s</p> </li> <li> <p>Install <code>nginx-ingress</code> with: <code>helm upgrade --install nginx ingress-nginx/ingress-nginx -n ingress-nginx --create-namespace</code></p> </li> <li>Install <code>cert-manager</code>. We recommend installing <code>cert-manager</code> through Jetpack, like so: </li> </ol> <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm repo update jetstack\nhelm upgrade --install \\\ncert-manager jetstack/cert-manager \\\n--namespace cert-manager \\\n--create-namespace \\\n--version v1.12.0 \\\n--set installCRDs=true\n</code></pre> <ol> <li> <p>Copy <code>cp ./chart/examples/k3s-hosted.yaml ./chart/my-config.yaml</code> to make local changes.</p> </li> <li> <p>Set the <code>ingress.host</code>, <code>ingress.cert_email</code> and <code>signing.host</code> fields in <code>./chart/my-config.yaml</code> to your host and domain</p> </li> <li> <p>Set the super-admin username and password, and mongodb username and password in <code>./chart/my-config.yaml</code></p> </li> <li> <p>Run with:</p> </li> </ol> <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/my-config.yaml btrix ./chart/\n</code></pre>"},{"location":"deploy/remote/#using-custom-storage","title":"Using Custom Storage","text":"<p>If you would like to use existing external storage, such an existing S3-compatible storage, also set the default storage, for example:</p> <pre><code>minio_local: false\nstorages:\n- name: \"default\"\naccess_key: &lt;access key&gt;\nsecret_key: &lt;secret key&gt;\nendpoint_url: \"https://s3.&lt;region&gt;.amazonaws.com/bucket/path/\"\n</code></pre> <p>Note that this setup is not limited to Amazon S3, but should work with any S3-compatible storage service.</p>"},{"location":"deploy/remote/#using-custom-mongodb","title":"Using Custom MongoDB","text":"<p>If you would like to use an externally hosted MongoDB, you can add the following config to point to a custom MongoDB instance.</p> <p>The <code>db_url</code> should follow the MongoDB Connection String Format which should include the username and password of the remote instance.</p> <pre><code>mongo_local: false\nmongo_auth:\ndb_url: mongodb+srv://...\n</code></pre>"},{"location":"deploy/remote/#cloud-deployment","title":"Cloud Deployment","text":"<p>There are also many ways to deploy Browsertrix Cloud on various cloud providers.</p> <p>To simplify this process, we are working on Ansible playbooks for setting up Browsertrix Cloud on commonly used infrastructure.</p>"},{"location":"deploy/remote/#ansible-deployment","title":"Ansible Deployment","text":"<p>Ansible makes the initial setup and configuration of your Browsertrix Cloud instance automated and repeatable. </p> <p>To use, you will need to install Ansible on your control computer and then you can use these to deploy to Browsertrix Cloud on remote and cloud environments.</p> <p>Currently, we provide playbooks for the following tested environments:</p> <ul> <li>DigitalOcean</li> <li>Microk8s</li> <li>k3s</li> </ul>"},{"location":"deploy/ansible/digitalocean/","title":"DigitalOcean","text":"<p>Playbook Path: ansible/playbooks/install_microk8s.yml</p> <p>This playbook provides an easy way to install Browsertrix Cloud on DigitalOcean. It automatically sets up Browsertrix with LetsEncrypt certificates.</p>"},{"location":"deploy/ansible/digitalocean/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a DigitalOcean Account where this will run.</li> <li>Create a DigitalOcean API Key which will need to be set in your terminal sessions environment variables <code>export DO_API_TOKEN</code> </li> <li><code>doctl</code> command line client configured (run <code>doctl auth init</code>)</li> <li>Create a DigitalOcean Spaces API Key which will also need to be set in your terminal sessions environment variables, which should be set as <code>DO_AWS_ACCESS_KEY</code> and <code>DO_AWS_SECRET_KEY</code></li> <li>Configure a DNS A Record and CNAME record.</li> <li>Have a working python and pip configuration through your OS Package Manager</li> </ul>"},{"location":"deploy/ansible/digitalocean/#install","title":"Install","text":"<ol> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix-cloud.git\ncd browsertrix-cloud\n</code></pre></p> </li> <li> <p>Install the Dependencies through pipenv <pre><code>cd ansible\npip install pipenv\npipenv install\npipenv shell\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them or pass them as extra variables as shown below. If you haven't configured <code>kubectl</code>, please enable the <code>configure_kube</code> option </p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook playbooks/do_setup.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\"\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/digitalocean/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook playbooks/do_setup.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain_name=\"yourdomain.com\" -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/digitalocean/#uninstall","title":"Uninstall","text":"<p>You can tear down your deployment through ansible as well. By default ansible will dump all the databases into your DO space. You can configure an option to disable this feature. </p> <pre><code>ansible-playbook playbooks/do_teardown.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\"\n</code></pre>"},{"location":"deploy/ansible/k3s/","title":"K3S","text":"<p>Playbook Path: ansible/playbooks/install_k3s.yml</p> <p>This playbook provides an easy way to install Browsertrix Cloud on a Linux box (tested on Rocky Linux 9). It automatically sets up Browsertrix with Let's Encrypt certificates.</p>"},{"location":"deploy/ansible/k3s/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a server / VPS where browsertrix will run.</li> <li>Configure a DNS A Record to point at your server's IP address.</li> <li>Make sure you can ssh to it, with a sudo user: ssh @ <li> <p>Install Ansible on your local machine (the control machine).</p> </li> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix-cloud.git\ncd browsertrix-cloud\n</code></pre></p> </li> <li> <p>Optional: Create a copy of the [inventory directory] and name it what you like (alternatively edit the sample files in place) <pre><code>cp -r ansible/inventory/sample-k3s ansible/inventory/my-deployment\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them to match your setup </p> </li> <li> <p>Change the hosts IP address in your just created inventory</p> </li> <li> <p>Copy the playbook into the root ansible directory: <pre><code>cp ansible/playbooks/install_k3s.yml ansible/install_k3s.yml\n</code></pre></p> </li> <li> <p>You may need to make modifications to the playbook itself based on your configuration. The playbook lists sections that can be removed or changed based on whether you'd like to install a multi-node or single-node k3s installation for your Browsertrix Cloud deployment. By default the playbook assumes you'll run in a single-node environment deploying directly to <code>localhost</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/my-deployment/hosts.ini install_k3s.yml\n</code></pre></p> </li>"},{"location":"deploy/ansible/k3s/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts playbooks/install_k3s.yml -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/microk8s/","title":"Microk8s","text":"<p>Playbook Path: ansible/playbooks/install_microk8s.yml</p> <p>This playbook provides an easy way to install Browsertrix Cloud on Ubuntu (tested on Jammy Jellyfish) and RedHat 9 (tested on Rocky Linux 9). It automatically sets up Browsertrix with Letsencrypt certificates.</p>"},{"location":"deploy/ansible/microk8s/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a server / VPS where browsertrix will run.</li> <li>Configure a DNS A Record to point at your server's IP address.</li> <li>Make sure you can ssh to it, with a sudo user: ssh @ <li>Install Ansible on your local machine (the control machine).</li>"},{"location":"deploy/ansible/microk8s/#install","title":"Install","text":"<ol> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix-cloud.git\ncd browsertrix-cloud\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them or pass them as extra variables as shown below. </p> </li> <li> <p>Add your IP address above to a new file called [inventory/hosts]</p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts playbooks/install_microk8s.yml -e host_ip=\"1.2.3.4\" -e domain_name=\"yourdomain.com\" -e your_user=\"your_vps_admin_user\"\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/microk8s/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts playbooks/install_microk8s.yml -e host_ip=\"1.2.3.4\" -e domain_name=\"yourdomain.com\" -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"develop/","title":"Developing Browsertrix Cloud","text":"<p>Browsertrix Cloud consists of a Python-based backend and TypeScript-based frontend.</p> <p>To develop Browsertrix Cloud, the system must first be deployed locally in a Kubernetes cluster.</p> <p>The deployment can then be further customized for local development.</p>"},{"location":"develop/#backend","title":"Backend","text":"<p>The backend is an API-only system, using the FastAPI framework. The latest API reference is available under ./api of a running cluster.</p> <p>At this time, the backend must be deployed in the Kubernetes cluster.</p>"},{"location":"develop/#frontend","title":"Frontend","text":"<p>The frontend UI is implemented in TypeScript, using the Lit framework and Shoelace component library.</p> <p>The static build of the frontend is bundled with nginx, but the frontend can be deployed locally in dev mode against an existing backend.</p> <p>See Running Frontend for more details.</p>"},{"location":"develop/docs/","title":"Writing Documentation","text":"<p>Our documentation is built with Material for MkDocs and configured via <code>mkdocs.yml</code> in the project root.</p> <p>The docs can be found in the <code>./docs</code> subdirectory.</p> <p>To build the docs locally, install Material for MkDocs with pip:</p> <pre><code>pip install mkdocs-material\n</code></pre> <p>In the project root directory run <code>mkdocs serve</code> to run a local version of the documentation site.</p> <p>The docs hosted on docs.browsertrix.cloud are created from the main branch of https://github.com/webrecorder/browsertrix-cloud</p>"},{"location":"develop/docs/#adding-new-pages","title":"Adding New Pages","text":"<ol> <li>Create a Markdown file in the directory of choice</li> <li>Add the newly created Markdown file to the <code>nav</code> value under the subsection as defined by the file's location in <code>mkdocs.yml</code>.</li> </ol>"},{"location":"develop/docs/#adding-icons","title":"Adding Icons","text":"<p>We typically use the Bootstrap icon set with our projects.  This set is quite expansive, and we don't add the entire set into our docs folder as most icons go unused.  If you wish to use an icon when writing documentation to refer to an icon present in part of the app, you may have to download the SVG file and add it to the repo.</p> <p>Icons are placed in the <code>docs/overrides/.icons/iconsetname/icon-name.svg</code> directory, and can be added in markdown files as <code>:iconsetname-icon-name:</code> accordingly.  For more information, see the Material for MKDocs page on Changing the logo and icons.</p>"},{"location":"develop/docs/#docs-style-guide","title":"Docs Style Guide","text":""},{"location":"develop/docs/#american-english","title":"American English","text":"<p>Webrecorder is a global team but we use American English when writing documentation and in-app copy.  Some basic rules to follow are:</p> <ol> <li>Swap the <code>s</code> for a <code>z</code> in words like categorize and pluralize.</li> <li>Remove the <code>u</code> from words like color and honor.</li> <li>Swap <code>tre</code> for <code>ter</code> in words like center.</li> <li>Numbers should be formatted with commas for seperation of values, using periods to denote decimals (e.g: 3,153.89, not 3 153,89).</li> </ol>"},{"location":"develop/docs/#oxford-commas","title":"Oxford Commas","text":"<p>In a list of three or more items, the list item proceeding the word \"and\" should have a comma placed after it clarifying that the final item in the list is not a part of the previous item.</p>"},{"location":"develop/docs/#example","title":"Example","text":"Use Don't use One, two, three, and four. One, two, three and four. Charles, Ada, and Alan. Charles, Ada and Alan."},{"location":"develop/docs/#acronyms","title":"Acronyms","text":"<p>Avoid using acronyms when reuse is not frequent enough to warrant space savings. When acronyms must be used, spell the full phrase first and include the acronym in parentheses <code>()</code> the first time it is used in each document.  This can be omitted for extremely common acronyms such as \"URL\" or \"HTTP\".</p>"},{"location":"develop/docs/#example_1","title":"Example","text":"<p>When running in a Virtual Machine (VM), use the....</p>"},{"location":"develop/docs/#headings","title":"Headings","text":"<p>All headings should be set in title case.</p>"},{"location":"develop/docs/#example_2","title":"Example","text":"<p>Indiana Jones and the Raiders of the Lost Ark</p>"},{"location":"develop/docs/#referencing-features-and-their-options","title":"Referencing Features and Their Options","text":"<p>Controls with multiple options should have their options referenced as <code>in-line code blocks</code>.</p> <p>Setting names referenced outside of a heading should be Title Cased and set in italics.</p> <p>Actions with text (buttons in the app) should also be Title Cased and set in italics.</p>"},{"location":"develop/docs/#example_3","title":"Example","text":"<p>Sets the day of the week for which crawls scheduled with a <code>Weekly</code> Frequency will run.</p>"},{"location":"develop/docs/#manual-word-wrapping","title":"Manual Word Wrapping","text":"<p>Do not manually wrap words by adding newlines when writing documentation.</p>"},{"location":"develop/docs/#code-block-syntax-highlighting","title":"Code Block Syntax Highlighting","text":"<p>Tag the language to be used for syntax highlighting.</p>"},{"location":"develop/docs/#example_4","title":"Example","text":"<pre><code> ```markdown\n example markdown code block text\n ```\n</code></pre> <p>For in-line code blocks, syntax highlighting should be added for all code-related usage by adding <code>#!language</code> to the start of all in-line code blocks. This is not required for paths or simply highlighting important text using in-line code blocks.</p>"},{"location":"develop/docs/#example_5","title":"Example","text":"<pre><code> `#!python range()`\n</code></pre> <p>Renders to: <code>range()</code></p>"},{"location":"develop/docs/#admonitions","title":"Admonitions","text":"<p>We use Admonitions in their collapsed state to offer additional context or tips that aren't relevant to all users reading the section. We use standard un-collapsable ones when we need to call attention to a specific point.</p> <p>There are a lot of different options provided by Material for MkDocs \u2014 So many in fact that we try to pair down their usage into the following categories.</p> Note <p>The default call-out, used to highlight something if there isn't a more relevant one \u2014 should generally be expanded by default but can be collapsable by the user if the note is long.</p> <p>Tip \u2014 May have a title stating the tip or best practice</p> <p>Used to highlight a point that is useful for everyone to understand about the documented subject \u2014 should be expanded and kept brief.</p> Info \u2014 Must have a title describing the context under which this information is useful <p>Used to deliver context-based content such as things that are dependant on operating system or environment \u2014 should be collapsed by default.</p> Example \u2014 Must have a title describing the content <p>Used to deliver additional information about a feature that could be useful in a specific circumstance or that might not otherwise be considered \u2014 should be collapsed by default.</p> Question \u2014 Must have a title phrased in the form of a question <p>Used to answer frequently asked questions about the documented subject \u2014 should be collapsed by default.</p> <p>Warning \u2014 Must have a title stating the warning</p> <p>Used to deliver important information \u2014 should always be expanded.</p> <p>Danger \u2014 Must have a title stating the warning</p> <p>Used to deliver information about serious unrecoverable actions such as deleting large amounts of data or resetting things \u2014 should always be expanded.</p>"},{"location":"develop/frontend-dev/","title":"Developing the Frontend UI","text":"<p>This guide explains how to run the Browsertrix Cloud frontend development server with Yarn.</p> <p>Instead of rebuilding the entire frontend image to view your UI changes, you can use the included local development server to access the frontend from your browser. This setup is ideal for rapid UI development that does not rely on any backend changes.</p>"},{"location":"develop/frontend-dev/#requirements","title":"Requirements","text":""},{"location":"develop/frontend-dev/#1-browsertrix-cloud-api-backend-already-in-a-kubernetes-cluster","title":"1. Browsertrix Cloud API backend already in a Kubernetes cluster","text":"<p>The frontend development server requires an existing backend that has been deployed locally or is in production. See Deploying Browsertrix Cloud.</p>"},{"location":"develop/frontend-dev/#2-nodejs-16-and-yarn-1","title":"2. Node.js \u226516 and Yarn 1","text":"<p>To check if you already have Node.js installed, run the following command in your command line terminal:</p> <pre><code>node --version\n</code></pre> <p>You should see a version number like <code>v18.12.1</code>. If you see a command line error instead of a version number, install Node.js before continuing.</p> What if my other project requires a different version of Node.js? <p>You can use Node Version Manager to install multiple Node.js versions and switch versions between projects.</p> <p>To check your Yarn installation:</p> <pre><code>yarn --version\n</code></pre> <p>You should see a version number like <code>1.22.19</code>. If you do not, install or upgrade Yarn.</p>"},{"location":"develop/frontend-dev/#quickstart","title":"Quickstart","text":"<p>From the command line, change your current working directory to <code>/frontend</code>:</p> <pre><code>cd frontend\n</code></pre> <p>Note</p> <p>From this point on, all commands in this guide should be run from the <code>frontend</code> directory.</p> <p>Install UI dependencies:</p> <pre><code>yarn install\n</code></pre> <p>Copy environment variables from the sample file:</p> <pre><code>cp sample.env.local .env.local\n</code></pre> <p>Update <code>API_BASE_URL</code> in <code>.env.local</code> to point to your backend API host. For example:</p> <pre><code>API_BASE_URL=http://dev.example.com\n</code></pre> <p>Note</p> <p>This setup assumes that your API endpoints are available under <code>/api</code>, which is the default configuration for the Browsertrix Cloud backend.</p> <p>If connecting to a local deployment cluster, set <code>API_BASE_URL</code> to:</p> <pre><code>API_BASE_URL=http://localhost:30870\n</code></pre> Port when using Minikube (on macOS) <p>When using Minikube on macOS, the port will not be 30870. Instead, Minikube opens a tunnel to a random port, obtained by running <code>minikube service browsertrix-cloud-frontend --url</code> in a separate terminal.</p> <p>Set API_BASE_URL to provided URL instead, eg. <code>API_BASE_URL=http://127.0.0.1:&lt;TUNNEL_PORT&gt;</code></p> <p>Start the frontend development server:</p> <pre><code>yarn start\n</code></pre> <p>This will open <code>localhost:9870</code> in a new tab in your default browser.</p> <p>Saving changes to files in <code>src</code> will automatically reload your browser window with the latest UI updates.</p> <p>To stop the development server type Ctrl+C into your command line terminal.</p>"},{"location":"develop/frontend-dev/#scripts","title":"Scripts","text":"<code>yarn &lt;name&gt;</code> <code>start</code> runs app in development server, reloading on file changes <code>test</code> runs tests in chromium with playwright <code>build-dev</code> bundles app and outputs it in <code>dist</code> directory <code>build</code> bundles app, optimized for production, and outputs it to <code>dist</code> <code>lint</code> find and fix auto-fixable javascript errors <code>format</code> formats js, html, and css files <code>localize:extract</code> generate XLIFF file to be translated <code>localize:build</code> output a localized version of strings/templates"},{"location":"develop/frontend-dev/#testing","title":"Testing","text":"<p>Tests assertions are written in Chai.</p> <p>To watch for file changes while running tests:</p> <pre><code>yarn test --watch\n</code></pre> <p>To run tests in multiple browsers:</p> <pre><code>yarn test --browsers chromium firefox webkit\n</code></pre>"},{"location":"develop/frontend-dev/#localization","title":"Localization","text":"<p>Wrap text or templates in the <code>msg</code> helper to make them localizable:</p> <pre><code>// import from @lit/localize:\nimport { msg } from \"@lit/localize\";\n// later, in the render function:\nrender() {\nreturn html`\n    &lt;button&gt;\n${msg(\"Click me\")}\n    &lt;/button&gt;\n  `\n}\n</code></pre> <p>Entire templates can be wrapped as well:</p> <pre><code>render() {\nreturn msg(html`\n    &lt;p&gt;Click the button&lt;/p&gt;\n    &lt;button&gt;Click me&lt;/button&gt;\n  `)\n}\n</code></pre> <p>See: https://lit.dev/docs/localization/overview/#message-types</p> <p>To add new languages:</p> <ol> <li>Add BCP 47 language tag to <code>targetLocales</code> in <code>lit-localize.json</code></li> <li>Run <code>yarn localize:extract</code> to generate new .xlf file in <code>/xliff</code></li> <li>Provide .xlf file to translation team</li> <li>Replace .xlf file once translated</li> <li>Run <code>yarn localize:build</code> bring translation into <code>src</code></li> </ol> <p>See: https://lit.dev/docs/localization/overview/#extracting-messages</p>"},{"location":"develop/local-dev-setup/","title":"Setup for Local Development","text":""},{"location":"develop/local-dev-setup/#installation","title":"Installation","text":"<p>First, see our Local Deployment guide for instructions on how to install the latest release with Kubernetes with Helm 3.</p>"},{"location":"develop/local-dev-setup/#local-dev-configuration","title":"Local Dev Configuration","text":"<p>The local deployment guide explains how to deploy Browsertrix Cloud with latest published images.</p> <p>However, if you are developing locally, you will need to use your local images instead.</p> <p>We recommend the following setup:</p> <ol> <li> <p>Copy the provided <code>./chart/examples/local-config.yaml</code> Helm configuration file to a separate file <code>local.yaml</code>, so that local changes to it will not be accidentally committed to git. From the root directory: <pre><code>cp ./chart/examples/local-config.yaml ./chart/local.yaml\n</code></pre></p> </li> <li> <p>Uncomment pull policies in <code>./chart/local.yaml</code>, which will ensure the local images are used: <pre><code>backend_pull_policy: 'Never'\nfrontend_pull_policy: 'Never'\n</code></pre></p> MicroK8S <p>For microk8s, the pull policies actually need to be set to <code>IfNotPresent</code> instead of <code>Never</code>:</p> <pre><code>backend_pull_policy: 'IfNotPresent'\nfrontend_pull_policy: 'IfNotPresent'\n</code></pre> <p>This will ensure images are pulled from the MicroK8S registry (configured in next section).</p> </li> <li> <p>Build the local backend and frontend images. The exact process depends on the Kubernetes environment you've selected in your initial deployment. Environment specific build instructions are as follows:</p> Docker Desktop <p>Rebuild the local images by running <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> scripts to build the images in the local Docker.</p> MicroK8S <p>MicroK8s uses its own container registry, running on port 32000.</p> <ol> <li> <p>Ensure the registry add-on is enabled by running <code>microk8s enable registry</code></p> </li> <li> <p>Set <code>export REGISTRY=localhost:32000/</code> and then run <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> to rebuild the images into the MicroK8S registry.</p> </li> <li> <p>In <code>./chart/local.yaml</code>, also uncomment the following lines to use the local images: <pre><code>backend_image: \"localhost:32000/webrecorder/browsertrix-backend:latest\"\nfrontend_image: \"localhost:32000/webrecorder/browsertrix-frontend:latest\"\n</code></pre></p> </li> </ol> Minikube <p>Minikube comes with its own image builder to update the images used in Minikube.</p> <p>To build the backend image, run:</p> <pre><code>minikube image build -t webrecorder/browsertrix-backend:latest ./backend\n</code></pre> <p>To build a local frontend image, run:</p> <pre><code>minikube image build -t webrecorder/browsertrix-frontend:latest ./frontend\n</code></pre> K3S <p>K3S uses <code>containerd</code> by default. To use local images, they need to be imported after rebuilding.</p> <ol> <li> <p>Rebuild the images with Docker by running by running <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> scripts. (Requires Docker to be installed as well).</p> </li> <li> <p>Serializer the images to .tar: <pre><code>docker save webrecorder/browsertrix-backend:latest &gt; ./backend.tar\ndocker save webrecorder/browsertrix-frontend:latest &gt; ./frontend.tar\n</code></pre></p> </li> <li> <p>Import images into k3s containerd: <pre><code>k3s ctr images import --base-name webrecorder/browsertrix-backend:latest ./backend.tar\nk3s ctr images import --base-name webrecorder/browsertrix-frontend:latest ./frontend.tar\n</code></pre></p> </li> </ol> </li> <li> <p>If you'd like to change other options, you can uncomment them as needed in <code>./chart/local.yaml</code> or add additional overrides from <code>./chart/values.yaml</code></p> <p>For example, to set a superuser email to <code>my_super_user_email@example.com</code> and password to <code>MySecretPassword!</code>, uncomment that block and set: <pre><code>superuser:\n# set this to enable a superuser admin\nemail: my_super_user_email@example.com\n# optional: if not set, automatically generated\n# change or remove this\npassword: MySecretPassword!\n</code></pre></p> </li> <li> <p>Once the images have been built and config changes made in <code>./chart/local.yaml</code>, </p> <p>the cluster can be re-deployed by running: <pre><code>helm upgrade --install -f ./chart/values.yaml \\\n-f ./chart/local.yaml btrix ./chart/\n</code></pre></p> MicroK8S <p>If using microk8s, the commend will be:</p> <pre><code>microk8s helm3 upgrade --install -f ./chart/values.yaml -f ./chart/local.yaml btrix ./chart/\n</code></pre> </li> </ol> <p>Refer back to the Local Development guide for additional information on running and debugging your local cluster.</p>"},{"location":"develop/local-dev-setup/#update-the-images","title":"Update the Images","text":"<p>After making any changes to backend code (in <code>./backend</code>) or frontend code (in <code>./frontend</code>), you'll need to rebuild the images as specified above, before running <code>helm upgrade ...</code> to re-deploy.</p> <p>Changes to settings in <code>./chart/local.yaml</code> can be deployed with <code>helm upgrade ...</code> directly.</p>"},{"location":"develop/local-dev-setup/#deploying-frontend-only","title":"Deploying Frontend Only","text":"<p>If you are just making changes to the frontend, you can also deploy the frontend separately using a dev server for quicker iteration.</p>"},{"location":"user-guide/","title":"Getting Started","text":""},{"location":"user-guide/#signup","title":"Signup","text":""},{"location":"user-guide/#invite-link","title":"Invite Link","text":"<p>If you have been sent an invite, enter a password and name to create a new account. Your account will be added to the organization you were invited to by an organization admin.</p>"},{"location":"user-guide/#open-registration","title":"Open Registration","text":"<p>If the server has enabled signups and you have been given a registration link, enter your email address, password, and name to create a new account. Your account will be added to the server's default organization.</p> <p>Note</p> <p>Names chosen on signup cannot be changed later.</p>"},{"location":"user-guide/#start-crawling","title":"Start Crawling!","text":"<p>A Crawl Workflow must be created in order to crawl websites automatically. A detailed list of all available workflow configuration options can be found on the Crawl Workflow Setup page.</p>"},{"location":"user-guide/browser-profiles/","title":"Browser Profiles","text":"<p>Browser profiles are saved instances of a web browsing session that can be reused to crawl websites as they were configued, with any cookies or saved login sessions. Using a pre-configured profile also means that content that can only be viewed by logged in users can be archived, without archiving the actual login credentials.</p> <p>Best practice: Create and use web archiving-specific accounts for crawling with browser profiles</p> <p>For the following reasons, we recommend creating dedicated accounts for archiving anything that is locked behind login credentials but otherwise public, especially on social media platforms.</p> <ul> <li> <p>While user names and passwords are not, the access tokens for logged in websites used in the browser profile creation process are stored by the server.</p> </li> <li> <p>Some websites may rate limit or lock accounts for reasons they deem to be suspicious, such as logging in from a new location or any crawling-related activity.</p> </li> <li> <p>While login information (username, password) is not archived, other data such as cookies, location, etc.. may be included in the resulting crawl (after all, personalized content is often the goal of sites that require credentials to view content).</p> </li> <li> <p>Due to nature of social media specifically, existing accounts may have personally identifiable information, even when accessing otherwise public content.</p> </li> </ul> <p>Of course, there are exceptions \u2014 such as when the goal is to archive personalized or private content accessible only from designated accounts.</p>"},{"location":"user-guide/browser-profiles/#creating-new-browser-profiles","title":"Creating New Browser Profiles","text":"<p>New browser profiles can be created on the Browser Profiles page by pressing the New Browser Profile button and providing a starting URL. Once in the profile creator, log in to any websites that should behave as logged in while crawling and accept any pop-ups that require interaction from the user to proceed with using the website.</p> <p>Press the Next button to save the browser profile with a Name and Description of what is logged in or otherwise notable about this browser session.</p>"},{"location":"user-guide/browser-profiles/#editing-existing-browser-profiles","title":"Editing Existing Browser Profiles","text":"<p>Sometimes websites will log users out or expire cookies after a period of time. In these cases, when crawling the browser profile can still be loaded but may not behave as it did when it was initially set up.</p> <p>To update the profile, go to the profile's details page and press the Edit Browser Profile button to load and interact with the sites that need to be re-configured. When finished, press the Save Browser Profile button to return to the profile's details page.</p>"},{"location":"user-guide/collections/","title":"Collections","text":"<p>Collections are the primary way of organizing and combining archived items into groups for presentation.</p> <p>Tip \u2014 Combining items from multiple sources</p> <p>If the crawler has not captured every resource or interaction on a webpage, the ArchiveWebpage browser extension can be used to manually capture missing content and upload it directly to your org.</p> <p>After adding the crawl and the upload to a collection, the content from both will become available in the replay viewer.</p>"},{"location":"user-guide/collections/#adding-content-to-collections","title":"Adding Content to Collections","text":"<p>Crawls and uploads can be added to a collection as part of the initial creation process or after creation by selecting Edit Collection from the collection's actions menu.</p> <p>A crawl workflow can also be set to automatically add any completed archived items to a collection in the workflow's settings.</p>"},{"location":"user-guide/collections/#sharing-collections","title":"Sharing Collections","text":"<p>Collections are private by default, but can be made public by marking them as sharable in the Metadata step of collection creation, or by toggling the Collection is Shareable switch in the share collection dialogue.</p> <p>After a collection has been made public, it can be shared with others using the public URL available in the share collection dialogue. The collection can also be embedded into other websites using the provided embed code. Unsharing the collection will break any previously shared links.</p> <p>For further resources on embedding archived web content into your own website, see the ReplayWebpage docs page on embedding.</p>"},{"location":"user-guide/crawl-workflows/","title":"Crawl Workflows","text":"<p>Crawl workflows consist of a list of configuration options that instruct the crawler what it should capture.</p>"},{"location":"user-guide/crawl-workflows/#creating-and-editing-crawl-workflows","title":"Creating and Editing Crawl Workflows","text":"<p>New crawl workflows can be created from the Crawling page. A detailed breakdown of available settings can be found here.</p>"},{"location":"user-guide/crawl-workflows/#running-crawl-workflows","title":"Running Crawl Workflows","text":"<p>Crawl workflows can be run from the actions menu of the workflow in the crawl workflow list, or by clicking the Run Crawl button on the workflow's details page.</p> <p>While crawling, the Watch Crawl page displays a list of queued URLs that will be visited, and streams the current state of the browser windows as they visit pages from the queue.</p> <p>Running a crawl workflow that has successfully run previously can be useful to capture content as it changes over time, or to run with an updated Crawl Scope.</p>"},{"location":"user-guide/crawl-workflows/#live-exclusion-editing","title":"Live Exclusion Editing","text":"<p>While exclusions can be set before running a crawl workflow, sometimes while crawling the crawler may find new parts of the site that weren't previously known about and shouldn't be crawled, or get stuck browsing parts of a website that automatically generate URLs known as \"crawler traps\".</p> <p>If the crawl queue is filled with URLs that should not be crawled, use the Edit Exclusions button on the Watch Crawl page to instruct the crawler what pages should be excluded from the queue.</p> <p>Exclusions added while crawling are applied to the same exclusion table saved in the workflow's settings and will be used the next time the crawl workflow is run unless they are manually removed.</p>"},{"location":"user-guide/crawl-workflows/#ending-a-crawl","title":"Ending a Crawl","text":"<p>If a crawl workflow is not crawling websites as intended it may be preferable to end crawling operations and update the crawl workflow's settings before trying again. There are two operations to end crawls, available both on the workflow's details page, or as part of the actions menu in the workflow list.</p>"},{"location":"user-guide/crawl-workflows/#stopping","title":"Stopping","text":"<p>Stopping a crawl will throw away the crawl queue but otherwise gracefully end the process and save anything that has been collected. Stopped crawls show up in the list of Archived Items and can be used like any other item in the app.</p>"},{"location":"user-guide/crawl-workflows/#canceling","title":"Canceling","text":"<p>Canceling a crawl will throw away all data collected and immediately end the process. Canceled crawls do not show up in the list of Archived Items, though a record of the runtime and workflow settings can be found in the crawl workflow's list of crawls.</p>"},{"location":"user-guide/org-settings/","title":"Org Settings","text":"<p>The Org Settings page is only available to organization Admins. It can be found in the main navigation menu.</p>"},{"location":"user-guide/org-settings/#org-information","title":"Org Information","text":"<p>This page lets you change the organization's name. This name must be unique.</p>"},{"location":"user-guide/org-settings/#members","title":"Members","text":"<p>This page lists all current members who have access to the organization, as well as any invited members who have not yet accepted an invitation to join the organization. In the Active Members table, Admins can change the permission level of all users in the organization, including other Admins. At least one user must be an Admin per-organization. Admins can also remove members by pressing the trash button.</p> <p>Admins can add new members to the organization by pressing the Invite New Member button. Enter the email address associated with the user, select the appropriate role, and press Invite to send a link to join the organization via email.</p> <p>Sent invites can be invalidated by pressing the trash button in the relevant Pending Invites table row.</p>"},{"location":"user-guide/workflow-setup/","title":"Crawl Workflow Setup","text":""},{"location":"user-guide/workflow-setup/#crawl-type","title":"Crawl Type","text":"<p>The first step in creating a new crawl workflow is to choose what type of crawl you want to run. Crawl types are fixed and cannot be converted or changed later.</p> <code>URL List</code> The crawler visits every URL specified in a list, and optionally every URL linked on those pages. <code>Seeded Crawl</code> The crawler automatically discovers and archives pages starting from a single seed URL."},{"location":"user-guide/workflow-setup/#scope","title":"Scope","text":""},{"location":"user-guide/workflow-setup/#list-of-urls","title":"List of URLs","text":"<p><code>URL List</code> <code>Seeded Crawl</code></p> <p>This list informs the crawler what pages it should capture as part of a URL List crawl.</p> <p>It is also available under the Additional URLs section for Seeded Crawls where it can accept arbitrary URLs that will be crawled regardless of other scoping rules.</p>"},{"location":"user-guide/workflow-setup/#include-any-linked-page","title":"Include Any Linked Page","text":"<p><code>URL List</code></p> <p>When enabled, the crawler will visit all the links it finds within each page defined in the List of URLs field.</p> Crawling tags &amp; search queries with URL List crawls <p>This setting can be useful for crawling the content of specific tags or searh queries. Specify the tag or search query URL(s) in the List of URLs field, e.g: <code>https://example.com/search?q=tag</code>, and enable Include Any Linked Page to crawl all the content present on that search query page.</p>"},{"location":"user-guide/workflow-setup/#crawl-start-url","title":"Crawl Start URL","text":"<p><code>Seeded Crawl</code></p> <p>This is the first page that the crawler will visit. It's important to set Crawl Start URL that accurately represents the scope of the pages you wish to crawl as the Start URL Scope selection will depend on this field's contents.</p> <p>You must specify the protocol (likely <code>http://</code> or <code>https://</code>) as a part of the URL entered into this field.</p>"},{"location":"user-guide/workflow-setup/#start-url-scope","title":"Start URL Scope","text":"<p><code>Seeded Crawl</code></p> <code>Hashtag Links Only</code> <p>This scope will ignore links that lead to other addresses such as <code>example.com/path</code> and will instead instruct the crawler to visit hashtag links such as <code>example.com/#linkedsection</code>.</p> <p>This scope can be useful for crawling certain web apps that may not use unique URLs for their pages.</p> <code>Pages in the Same Directory</code> This scope will only crawl pages in the same directory as the Crawl Start URL. If <code>example.com/path</code> is set as the Crawl Start URL, <code>example.com/path/path2</code> will be crawled but <code>example.com/path3</code> will not. <code>Pages on This Domain</code> This scope will crawl all pages on the domain entered as the Crawl Start URL however it will ignore subdomains such as <code>subdomain.example.com</code>. <code>Pages on This Domain and Subdomains</code> This scope will crawl all pages on the domain and any subdomains found. If <code>example.com</code> is set as the Crawl Start URL, both pages on <code>example.com</code> and <code>subdomain.example.com</code> will be crawled. <code>Custom Page Prefix</code> This scope will crawl all pages that begin with the Crawl Start URL as well as pages from any URL that begin with the URLs listed in <code>Extra URLs in Scope</code>"},{"location":"user-guide/workflow-setup/#max-depth","title":"Max Depth","text":"<p><code>Seeded Crawl</code></p> <p>Only shown with a Start URL Scope of <code>Pages on This Domain</code> and above, the Max Depth setting instructs the crawler to stop visiting new links past a specified depth.</p>"},{"location":"user-guide/workflow-setup/#extra-urls-in-scope","title":"Extra URLs in Scope","text":"<p><code>Seeded Crawl</code></p> <p>Only shown with a Start URL Scope of <code>Custom Page Prefix</code>, this field accepts additional URLs or domains that will be crawled if URLs that lead to them are found.</p> <p>This can be useful for crawling websites that span multiple domains such as <code>example.org</code> and <code>example.net</code></p>"},{"location":"user-guide/workflow-setup/#include-any-linked-page-one-hop-out","title":"Include Any Linked Page (\"one hop out\")","text":"<p><code>Seeded Crawl</code></p> <p>When enabled, the crawler will visit all the links it finds within each page, regardless of the Start URL Scope setting.</p> <p>This can be useful for capturing links on a page that lead outside the website that is being crawled but should still be included in the archive for context.</p>"},{"location":"user-guide/workflow-setup/#check-for-sitemap","title":"Check For Sitemap","text":"<p><code>Seeded Crawl</code></p> <p>When enabled, the crawler will check for a sitemap at /sitemap.xml and use it to discover pages to crawl if found. It will not crawl pages found in the sitemap that do not meet the crawl's scope settings or limits.</p> <p>This can be useful for discovering and capturing pages on a website that aren't linked to from the seed and which might not otherwise be captured.</p>"},{"location":"user-guide/workflow-setup/#exclusions","title":"Exclusions","text":"<p><code>URL List</code> <code>Seeded Crawl</code></p> <p>The exclusions table will instruct the crawler to ignore links it finds on pages where all or part of the link matches an exclusion found in the table. The table is only available in URL List crawls when Include Any Linked Page is enabled.</p> <p>This can be useful for avoiding crawler traps \u2014 sites that may automatically generate pages such as calendars or filter options \u2014 or other pages that should not be crawled according to their URL.</p> <code>Matches text</code> <p>Will perform simple matching of entered text and exclude all URLs where matching text is found.</p> <p>e.g: If <code>about</code> is entered, <code>example.com/aboutme/</code> will not be crawled.</p> <code>Regex</code> <p>Regular expressions (Regex) can also be used to perform more complex matching.</p> <p>e.g: If <code>\\babout\\/?\\b</code> is entered, <code>example.com/about/</code> will not be crawled however <code>example.com/aboutme/</code> will be crawled.</p>"},{"location":"user-guide/workflow-setup/#limits","title":"Limits","text":""},{"location":"user-guide/workflow-setup/#max-pages","title":"Max Pages","text":"<p>Adds a hard limit on the number of pages that will be crawled. The crawl will be gracefully stopped after this limit is reached.</p>"},{"location":"user-guide/workflow-setup/#crawl-time-limit","title":"Crawl Time Limit","text":"<p>The crawl will be gracefully stopped after this set period of time.</p>"},{"location":"user-guide/workflow-setup/#crawl-size-limit","title":"Crawl Size Limit","text":"<p>The crawl will be gracefully stopped after reaching this set size in GB.</p>"},{"location":"user-guide/workflow-setup/#crawler-instances","title":"Crawler Instances","text":"<p>Increasing the amount of crawler instances will speed up crawls by using additional browser windows to capture more pages in parallel. This will also increase the amount of traffic sent to the website and may result in a higher chance of getting rate limited.</p>"},{"location":"user-guide/workflow-setup/#page-load-timeout","title":"Page Load Timeout","text":"<p>Limits amount of time to wait for a page to load. Behaviors will run after this timeout only if the page is partially or fully loaded.</p>"},{"location":"user-guide/workflow-setup/#behavior-timeout","title":"Behavior Timeout","text":"<p>Limits how long behaviors can run on each page.</p>"},{"location":"user-guide/workflow-setup/#auto-scroll-behavior","title":"Auto Scroll Behavior","text":"<p>When enabled, the browser will automatically scroll to the end of the page.</p>"},{"location":"user-guide/workflow-setup/#delay-before-next-page","title":"Delay Before Next Page","text":"<p>Waits on the page for a set period of time after any behaviors have finished running. This can be helpful to avoid rate limiting however it will slow down your crawl.</p>"},{"location":"user-guide/workflow-setup/#browser-settings","title":"Browser Settings","text":""},{"location":"user-guide/workflow-setup/#browser-profile","title":"Browser Profile","text":"<p>Sets the Browser Profile to be used for this crawl.</p>"},{"location":"user-guide/workflow-setup/#block-ads-by-domain","title":"Block Ads by Domain","text":"<p>Will prevent any content from the domains listed in Steven Black's Unified Hosts file (ads &amp; malware) from being captured by the crawler.</p>"},{"location":"user-guide/workflow-setup/#language","title":"Language","text":"<p>Sets the browser's language setting. Useful for crawling websites that detect the browser's language setting and serve content accordingly.</p>"},{"location":"user-guide/workflow-setup/#scheduling","title":"Scheduling","text":""},{"location":"user-guide/workflow-setup/#crawl-schedule-type","title":"Crawl Schedule Type","text":"<code>Run Immediately on Save</code> When selected, the crawl will run immediately as configured. It will not run again unless manually instructed. <code>Run on a Recurring Basis</code> When selected, additional configuration options for instructing the system when to run the crawl will be shown. If a crawl is already running when the schedule is set to activate it, the scheduled crawl will not run. <code>No Schedule</code> When selected, the configuration options that have been set will be saved but the system will not do anything with them unless manually instructed."},{"location":"user-guide/workflow-setup/#frequency","title":"Frequency","text":"<p>Set how often a scheduled crawl will run.</p>"},{"location":"user-guide/workflow-setup/#day","title":"Day","text":"<p>Sets the day of the week for which crawls scheduled with a <code>Weekly</code> Frequency will run.</p>"},{"location":"user-guide/workflow-setup/#date","title":"Date","text":"<p>Sets the date of the month for which crawls scheduled with a <code>Monthly</code> Frequency will run.</p>"},{"location":"user-guide/workflow-setup/#start-time","title":"Start Time","text":"<p>Sets the time that the scheduled crawl will start according to your current timezone.</p>"},{"location":"user-guide/workflow-setup/#also-run-a-crawl-immediately-on-save","title":"Also Run a Crawl Immediately On Save","text":"<p>When enabled, a crawl will run immediately on save as if the <code>Run Immediately on Save</code> Crawl Schedule Type was selected, in addition to scheduling a crawl to run according to the above settings.</p>"},{"location":"user-guide/workflow-setup/#metadata","title":"Metadata","text":""},{"location":"user-guide/workflow-setup/#name","title":"Name","text":"<p>Allows a custom name to be set for the workflow. If no name is set, the workflow's name will be set to the Crawl Start URL. For URL List crawls, the workflow's name will be set to the first URL present in the List of URLs field, with an added <code>(+x)</code> where <code>x</code> represents the total number of URLs in the list.</p>"},{"location":"user-guide/workflow-setup/#description","title":"Description","text":"<p>Leave optional notes about the workflow's configuration.</p>"},{"location":"user-guide/workflow-setup/#tags","title":"Tags","text":"<p>Apply tags to the workflow. Tags applied to the workflow will propigate to every crawl created with it at the time of crawl creation.</p>"},{"location":"user-guide/workflow-setup/#collection-auto-add","title":"Collection Auto-Add","text":"<p>Search for and specify collections that this crawl workflow should automatically add content to as soon as crawling finishes. Canceled and Failed crawls will not be automatically added to collections.</p>"}]}